{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安裝 kears-crontrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
      "  Cloning https://www.github.com/keras-team/keras-contrib.git to /private/var/folders/46/b7dzk4mn6g54qzptv608w7d00000gn/T/pip-t1_7py2z-build\n",
      "Requirement already satisfied: keras in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from keras-contrib==2.0.8)\n",
      "Requirement already satisfied: pyyaml in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8)\n",
      "Installing collected packages: keras-contrib\n",
      "  Running setup.py install for keras-contrib ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed keras-contrib-2.0.8\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, dataset_name, img_res=(128, 128)):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.img_res = img_res\n",
    "\n",
    "    def load_data(self, domain, batch_size=1, is_testing=False):\n",
    "        data_type = \"train%s\" % domain if not is_testing else \"test%s\" % domain\n",
    "        path = glob('./datasets/%s/%s/*' % (self.dataset_name, data_type))\n",
    "        #print(path)\n",
    "        batch_images = np.random.choice(path, size=batch_size)\n",
    "\n",
    "        imgs = []\n",
    "        for img_path in batch_images:\n",
    "            img = self.imread(img_path)\n",
    "            if not is_testing:\n",
    "                img = scipy.misc.imresize(img, self.img_res)\n",
    "\n",
    "                if np.random.random() > 0.5:\n",
    "                    img = np.fliplr(img)\n",
    "            else:\n",
    "                img = scipy.misc.imresize(img, self.img_res)\n",
    "            imgs.append(img)\n",
    "\n",
    "        imgs = np.array(imgs)/127.5 - 1.\n",
    "\n",
    "        return imgs\n",
    "\n",
    "    def load_batch(self, batch_size=1, is_testing=False):\n",
    "        data_type = \"train\" if not is_testing else \"val\"\n",
    "        path_A = glob('./datasets/%s/%sA/*' % (self.dataset_name, data_type))\n",
    "        path_B = glob('./datasets/%s/%sB/*' % (self.dataset_name, data_type))\n",
    "        #print(path_A)\n",
    "        self.n_batches = int(min(len(path_A), len(path_B)) / batch_size)\n",
    "        total_samples = self.n_batches * batch_size\n",
    "\n",
    "        # Sample n_batches * batch_size from each path list so that model sees all\n",
    "        # samples from both domains\n",
    "        path_A = np.random.choice(path_A, total_samples, replace=False)\n",
    "        path_B = np.random.choice(path_B, total_samples, replace=False)\n",
    "\n",
    "        for i in range(self.n_batches-1):\n",
    "            batch_A = path_A[i*batch_size:(i+1)*batch_size]\n",
    "            batch_B = path_B[i*batch_size:(i+1)*batch_size]\n",
    "            imgs_A, imgs_B = [], []\n",
    "            for img_A, img_B in zip(batch_A, batch_B):\n",
    "                img_A = self.imread(img_A)\n",
    "                img_B = self.imread(img_B)\n",
    "\n",
    "                img_A = scipy.misc.imresize(img_A, self.img_res)\n",
    "                img_B = scipy.misc.imresize(img_B, self.img_res)\n",
    "\n",
    "                if not is_testing and np.random.random() > 0.5:\n",
    "                        img_A = np.fliplr(img_A)\n",
    "                        img_B = np.fliplr(img_B)\n",
    "\n",
    "                imgs_A.append(img_A)\n",
    "                imgs_B.append(img_B)\n",
    "\n",
    "            imgs_A = np.array(imgs_A)/127.5 - 1.\n",
    "            imgs_B = np.array(imgs_B)/127.5 - 1.\n",
    "\n",
    "            yield imgs_A, imgs_B\n",
    "\n",
    "    def load_img(self, path):\n",
    "        img = self.imread(path)\n",
    "        img = scipy.misc.imresize(img, self.img_res)\n",
    "        img = img/127.5 - 1.\n",
    "        return img[np.newaxis, :, :, :]\n",
    "\n",
    "    def imread(self, path):\n",
    "        return scipy.misc.imread(path, mode='RGB').astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取馬對斑馬轉換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DataLoader('horse2zebra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定網路參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras_contrib.layers.normalization import InstanceNormalization\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# Input shape\n",
    "img_rows = 128\n",
    "img_cols = 128\n",
    "channels = 3\n",
    "img_shape = (img_rows, img_cols, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "    def conv2d(layer_input, filters, f_size=4):\n",
    "        \"\"\"Layers used during downsampling\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        d = InstanceNormalization()(d)\n",
    "        return d\n",
    "\n",
    "    def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "        \"\"\"Layers used during upsampling\"\"\"\n",
    "        u = UpSampling2D(size=2)(layer_input)\n",
    "        u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "        if dropout_rate:\n",
    "            u = Dropout(dropout_rate)(u)\n",
    "        u = InstanceNormalization()(u)\n",
    "        u = Concatenate()([u, skip_input])\n",
    "        return u\n",
    "\n",
    "    # Image input\n",
    "    d0 = Input(shape=img_shape)\n",
    "\n",
    "    # Downsampling\n",
    "    d1 = conv2d(d0, gf)\n",
    "    d2 = conv2d(d1, gf*2)\n",
    "    d3 = conv2d(d2, gf*4)\n",
    "    d4 = conv2d(d3, gf*8)\n",
    "\n",
    "    # Upsampling\n",
    "    u1 = deconv2d(d4, d3, gf*4)\n",
    "    u2 = deconv2d(u1, d2, gf*2)\n",
    "    u3 = deconv2d(u2, d1, gf)\n",
    "\n",
    "    u4 = UpSampling2D(size=2)(u3)\n",
    "    output_img = Conv2D(channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n",
    "\n",
    "    return Model(d0, output_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立鑑別器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "\n",
    "    def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
    "        \"\"\"Discriminator layer\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        if normalization:\n",
    "            d = InstanceNormalization()(d)\n",
    "        return d\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "\n",
    "    d1 = d_layer(img, df, normalization=False)\n",
    "    d2 = d_layer(d1, df*2)\n",
    "    d3 = d_layer(d2, df*4)\n",
    "    d4 = d_layer(d3, df*8)\n",
    "\n",
    "    validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立訓練過程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, batch_size=1, sample_interval=50):\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # Adversarial loss ground truths\n",
    "    valid = np.ones((batch_size,) + disc_patch)\n",
    "    fake = np.zeros((batch_size,) + disc_patch)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_i, (imgs_A, imgs_B) in enumerate(data_loader.load_batch(batch_size)):\n",
    "\n",
    "            # ----------------------\n",
    "            #  Train Discriminators\n",
    "            # ----------------------\n",
    "\n",
    "            # Translate images to opposite domain\n",
    "            fake_B = g_AB.predict(imgs_A)\n",
    "            fake_A = g_BA.predict(imgs_B)\n",
    "\n",
    "            # Train the discriminators (original images = real / translated = Fake)\n",
    "            dA_loss_real = d_A.train_on_batch(imgs_A, valid)\n",
    "            dA_loss_fake = d_A.train_on_batch(fake_A, fake)\n",
    "            dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
    "\n",
    "            dB_loss_real = d_B.train_on_batch(imgs_B, valid)\n",
    "            dB_loss_fake = d_B.train_on_batch(fake_B, fake)\n",
    "            dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
    "\n",
    "            # Total disciminator loss\n",
    "            d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
    "\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "\n",
    "            # Train the generators\n",
    "            g_loss = combined.train_on_batch([imgs_A, imgs_B],\n",
    "                                                    [valid, valid,\n",
    "                                                    imgs_A, imgs_B,\n",
    "                                                    imgs_A, imgs_B])\n",
    "\n",
    "            elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n",
    "                                                                    % ( epoch, epochs,\n",
    "                                                                        batch_i, data_loader.n_batches,\n",
    "                                                                        d_loss[0], 100*d_loss[1],\n",
    "                                                                        g_loss[0],\n",
    "                                                                        np.mean(g_loss[1:3]),\n",
    "                                                                        np.mean(g_loss[3:5]),\n",
    "                                                                        np.mean(g_loss[5:6]),\n",
    "                                                                        elapsed_time))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if batch_i % sample_interval == 0:\n",
    "                sample_images(epoch, batch_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 圖片取樣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(epoch, batch_i):\n",
    "    os.makedirs('images/%s' % dataset_name, exist_ok=True)\n",
    "    r, c = 2, 3\n",
    "\n",
    "    imgs_A = data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n",
    "    imgs_B = data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n",
    "    print(img_A)\n",
    "    # Demo (for GIF)\n",
    "    #imgs_A = data_loader.load_img('datasets/apple2orange/testA/n07740461_1541.jpg')\n",
    "    #imgs_B = data_loader.load_img('datasets/apple2orange/testB/n07749192_4241.jpg')\n",
    "\n",
    "    # Translate images to the other domain\n",
    "    fake_B = g_AB.predict(imgs_A)\n",
    "    fake_A = g_BA.predict(imgs_B)\n",
    "    # Translate back to original domain\n",
    "    reconstr_A = g_BA.predict(fake_B)\n",
    "    reconstr_B = g_AB.predict(fake_A)\n",
    "\n",
    "    gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    titles = ['Original', 'Translated', 'Reconstructed']\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt])\n",
    "            axs[i, j].set_title(titles[j])\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/%s/%d_%d.png\" % (dataset_name, epoch, batch_i))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建構模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# Configure data loader\n",
    "dataset_name = 'horse2zebra'\n",
    "data_loader = DataLoader(dataset_name=dataset_name,\n",
    "                              img_res=(img_rows, img_cols))\n",
    "\n",
    "\n",
    "# Calculate output shape of D (PatchGAN)\n",
    "patch = int(img_rows / 2**4)\n",
    "disc_patch = (patch, patch, 1)\n",
    "\n",
    "# Number of filters in the first layer of G and D\n",
    "gf = 32\n",
    "df = 64\n",
    "\n",
    "# Loss weights\n",
    "lambda_cycle = 10.0                    # Cycle-consistency loss\n",
    "lambda_id = 0.1 * lambda_cycle    # Identity loss\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminators\n",
    "d_A = build_discriminator()\n",
    "d_B = build_discriminator()\n",
    "d_A.compile(loss='mse',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "d_B.compile(loss='mse',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "#-------------------------\n",
    "# Construct Computational\n",
    "#   Graph of Generators\n",
    "#-------------------------\n",
    "\n",
    "# Build the generators\n",
    "g_AB = build_generator()\n",
    "g_BA = build_generator()\n",
    "\n",
    "# Input images from both domains\n",
    "img_A = Input(shape=img_shape)\n",
    "img_B = Input(shape=img_shape)\n",
    "\n",
    "# Translate images to the other domain\n",
    "fake_B = g_AB(img_A)\n",
    "fake_A = g_BA(img_B)\n",
    "# Translate images back to original domain\n",
    "reconstr_A = g_BA(fake_B)\n",
    "reconstr_B = g_AB(fake_A)\n",
    "# Identity mapping of images\n",
    "img_A_id = g_BA(img_A)\n",
    "img_B_id = g_AB(img_B)\n",
    "\n",
    "# For the combined model we will only train the generators\n",
    "d_A.trainable = False\n",
    "d_B.trainable = False\n",
    "\n",
    "# Discriminators determines validity of translated images\n",
    "valid_A = d_A(fake_A)\n",
    "valid_B = d_B(fake_B)\n",
    "\n",
    "# Combined model trains generators to fool discriminators\n",
    "combined = Model(inputs=[img_A, img_B],\n",
    "                      outputs=[ valid_A, valid_B,\n",
    "                                reconstr_A, reconstr_B,\n",
    "                                img_A_id, img_B_id ])\n",
    "combined.compile(loss=['mse', 'mse',\n",
    "                       'mae', 'mae',\n",
    "                       'mae', 'mae'],\n",
    "                    loss_weights=[  1, 1,\n",
    "                                    lambda_cycle, lambda_cycle,\n",
    "                                    lambda_id, lambda_id ],\n",
    "                    optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 開始訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/scipy/misc/pilutil.py:480: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/scipy/misc/pilutil.py:483: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n",
      "/Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/keras/engine/training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/20] [Batch 0/31] [D loss: 30.473705, acc:  10%] [G loss: 48.588200, adv: 16.974632, recon: 0.647205, id: 0.800721] time: 0:00:28.406611 \n",
      "Tensor(\"input_5:0\", shape=(?, 128, 128, 3), dtype=float32)\n",
      "[Epoch 0/20] [Batch 1/31] [D loss: 8.922375, acc:  15%] [G loss: 28.108395, adv: 7.612333, recon: 0.544633, id: 0.947274] time: 0:00:40.051470 \n",
      "[Epoch 0/20] [Batch 2/31] [D loss: 14.264787, acc:   6%] [G loss: 112.850899, adv: 49.705875, recon: 0.576540, id: 0.816267] time: 0:00:51.601959 \n",
      "[Epoch 0/20] [Batch 3/31] [D loss: 11.656817, acc:  13%] [G loss: 35.564556, adv: 11.348845, recon: 0.546520, id: 0.941505] time: 0:01:02.694095 \n",
      "[Epoch 0/20] [Batch 4/31] [D loss: 11.041611, acc:  15%] [G loss: 14.775379, adv: 0.843940, recon: 0.558030, id: 0.954537] time: 0:01:13.907103 \n",
      "[Epoch 0/20] [Batch 5/31] [D loss: 1.127777, acc:  29%] [G loss: 16.983524, adv: 1.649649, recon: 0.582375, id: 1.059489] time: 0:01:25.149874 \n",
      "[Epoch 0/20] [Batch 6/31] [D loss: 0.772206, acc:  54%] [G loss: 13.853879, adv: 0.771409, recon: 0.521627, id: 0.911694] time: 0:01:36.333831 \n",
      "[Epoch 0/20] [Batch 7/31] [D loss: 0.410238, acc:  51%] [G loss: 14.154687, adv: 0.613325, recon: 0.545194, id: 0.930898] time: 0:01:47.639997 \n",
      "[Epoch 0/20] [Batch 8/31] [D loss: 0.261967, acc:  64%] [G loss: 13.294575, adv: 0.739129, recon: 0.487195, id: 0.995147] time: 0:01:58.584937 \n",
      "[Epoch 0/20] [Batch 9/31] [D loss: 0.218504, acc:  71%] [G loss: 13.564354, adv: 0.869532, recon: 0.490051, id: 0.992580] time: 0:02:09.415066 \n",
      "[Epoch 0/20] [Batch 10/31] [D loss: 0.345368, acc:  60%] [G loss: 12.204577, adv: 0.717246, recon: 0.444960, id: 1.018596] time: 0:02:20.185515 \n",
      "Tensor(\"input_5:0\", shape=(?, 128, 128, 3), dtype=float32)\n",
      "[Epoch 0/20] [Batch 11/31] [D loss: 0.250804, acc:  67%] [G loss: 11.799218, adv: 0.866370, recon: 0.403732, id: 0.955034] time: 0:02:31.414376 \n",
      "[Epoch 0/20] [Batch 12/31] [D loss: 0.265220, acc:  65%] [G loss: 12.535759, adv: 0.788232, recon: 0.454123, id: 0.918122] time: 0:02:42.973540 \n",
      "[Epoch 0/20] [Batch 13/31] [D loss: 0.251691, acc:  66%] [G loss: 10.626082, adv: 0.740950, recon: 0.362256, id: 0.914646] time: 0:02:54.151709 \n",
      "[Epoch 0/20] [Batch 14/31] [D loss: 0.187427, acc:  75%] [G loss: 11.284102, adv: 0.931126, recon: 0.373669, id: 0.962408] time: 0:03:05.851865 \n",
      "[Epoch 0/20] [Batch 15/31] [D loss: 0.215916, acc:  69%] [G loss: 11.620317, adv: 0.838625, recon: 0.401028, id: 0.933593] time: 0:03:17.141863 \n",
      "[Epoch 0/20] [Batch 16/31] [D loss: 0.274850, acc:  61%] [G loss: 10.730701, adv: 0.814658, recon: 0.358421, id: 0.977805] time: 0:03:28.838242 \n",
      "[Epoch 0/20] [Batch 17/31] [D loss: 0.230157, acc:  67%] [G loss: 11.233678, adv: 0.735535, recon: 0.398308, id: 0.887943] time: 0:03:40.290223 \n",
      "[Epoch 0/20] [Batch 18/31] [D loss: 0.284621, acc:  65%] [G loss: 11.080382, adv: 0.987663, recon: 0.363708, id: 0.847090] time: 0:03:51.378036 \n",
      "[Epoch 0/20] [Batch 19/31] [D loss: 0.291540, acc:  63%] [G loss: 10.810298, adv: 0.875930, recon: 0.355251, id: 0.987762] time: 0:04:02.836512 \n",
      "[Epoch 0/20] [Batch 20/31] [D loss: 0.340809, acc:  59%] [G loss: 10.383327, adv: 0.759262, recon: 0.352310, id: 0.896299] time: 0:04:13.922973 \n",
      "Tensor(\"input_5:0\", shape=(?, 128, 128, 3), dtype=float32)\n",
      "[Epoch 0/20] [Batch 21/31] [D loss: 0.265911, acc:  65%] [G loss: 10.604132, adv: 0.759672, recon: 0.364855, id: 0.890122] time: 0:04:25.859667 \n",
      "[Epoch 0/20] [Batch 22/31] [D loss: 0.250973, acc:  63%] [G loss: 10.693823, adv: 0.851171, recon: 0.362125, id: 0.798101] time: 0:04:36.909901 \n",
      "[Epoch 0/20] [Batch 23/31] [D loss: 0.339043, acc:  55%] [G loss: 10.503574, adv: 0.776826, recon: 0.356817, id: 0.837406] time: 0:04:48.445368 \n",
      "[Epoch 0/20] [Batch 24/31] [D loss: 0.342358, acc:  54%] [G loss: 10.548382, adv: 0.747023, recon: 0.365564, id: 0.877470] time: 0:05:00.517699 \n",
      "[Epoch 0/20] [Batch 25/31] [D loss: 0.280992, acc:  58%] [G loss: 10.976439, adv: 1.079263, recon: 0.355227, id: 0.919473] time: 0:05:11.551767 \n",
      "[Epoch 0/20] [Batch 26/31] [D loss: 0.494628, acc:  41%] [G loss: 10.608747, adv: 1.014933, recon: 0.348194, id: 0.797782] time: 0:05:22.622630 \n",
      "[Epoch 0/20] [Batch 27/31] [D loss: 0.434476, acc:  47%] [G loss: 9.881456, adv: 0.846169, recon: 0.320330, id: 0.850906] time: 0:05:33.875719 \n",
      "[Epoch 0/20] [Batch 28/31] [D loss: 1.086252, acc:  20%] [G loss: 10.633446, adv: 0.913021, recon: 0.358954, id: 0.698140] time: 0:05:45.234569 \n",
      "[Epoch 0/20] [Batch 29/31] [D loss: 0.594156, acc:  29%] [G loss: 9.987176, adv: 0.842642, recon: 0.335218, id: 0.873211] time: 0:05:56.262820 \n",
      "[Epoch 1/20] [Batch 0/31] [D loss: 0.542155, acc:  30%] [G loss: 10.258024, adv: 0.737340, recon: 0.357745, id: 0.819672] time: 0:06:07.372669 \n",
      "Tensor(\"input_5:0\", shape=(?, 128, 128, 3), dtype=float32)\n",
      "[Epoch 1/20] [Batch 1/31] [D loss: 0.344354, acc:  51%] [G loss: 11.325629, adv: 0.707864, recon: 0.416301, id: 0.775560] time: 0:06:18.950203 \n",
      "[Epoch 1/20] [Batch 2/31] [D loss: 0.424901, acc:  38%] [G loss: 10.332106, adv: 0.665767, recon: 0.367145, id: 0.849669] time: 0:06:30.800688 \n",
      "[Epoch 1/20] [Batch 3/31] [D loss: 0.458394, acc:  39%] [G loss: 10.415772, adv: 0.764383, recon: 0.363521, id: 0.745606] time: 0:06:42.583299 \n",
      "[Epoch 1/20] [Batch 4/31] [D loss: 0.456742, acc:  38%] [G loss: 8.894803, adv: 0.608009, recon: 0.297387, id: 0.793758] time: 0:06:56.119871 \n",
      "[Epoch 1/20] [Batch 5/31] [D loss: 0.335510, acc:  45%] [G loss: 9.229041, adv: 0.751559, recon: 0.298423, id: 0.843347] time: 0:07:08.821822 \n",
      "[Epoch 1/20] [Batch 6/31] [D loss: 0.337956, acc:  49%] [G loss: 9.360074, adv: 0.709407, recon: 0.308255, id: 0.892409] time: 0:07:20.603522 \n",
      "[Epoch 1/20] [Batch 7/31] [D loss: 0.290945, acc:  53%] [G loss: 8.674617, adv: 0.727558, recon: 0.271697, id: 0.906607] time: 0:07:31.966280 \n",
      "[Epoch 1/20] [Batch 8/31] [D loss: 0.523431, acc:  26%] [G loss: 8.639011, adv: 0.620466, recon: 0.293115, id: 0.754451] time: 0:07:43.669124 \n",
      "[Epoch 1/20] [Batch 9/31] [D loss: 0.356344, acc:  48%] [G loss: 9.507456, adv: 0.821954, recon: 0.305784, id: 0.884064] time: 0:07:57.292907 \n",
      "[Epoch 1/20] [Batch 10/31] [D loss: 0.439513, acc:  35%] [G loss: 9.310958, adv: 0.710985, recon: 0.293836, id: 1.080310] time: 0:08:11.113984 \n",
      "Tensor(\"input_5:0\", shape=(?, 128, 128, 3), dtype=float32)\n",
      "[Epoch 1/20] [Batch 11/31] [D loss: 0.500296, acc:  32%] [G loss: 9.671236, adv: 0.743567, recon: 0.320094, id: 0.903689] time: 0:08:25.978533 \n",
      "[Epoch 1/20] [Batch 12/31] [D loss: 0.412706, acc:  40%] [G loss: 9.340431, adv: 0.718379, recon: 0.312608, id: 0.841004] time: 0:08:39.884098 \n",
      "[Epoch 1/20] [Batch 13/31] [D loss: 0.506126, acc:  29%] [G loss: 8.936888, adv: 0.723468, recon: 0.297745, id: 0.784830] time: 0:08:53.879776 \n",
      "[Epoch 1/20] [Batch 14/31] [D loss: 0.373903, acc:  44%] [G loss: 9.022249, adv: 0.776121, recon: 0.302424, id: 0.769601] time: 0:09:06.935461 \n",
      "[Epoch 1/20] [Batch 15/31] [D loss: 0.467169, acc:  34%] [G loss: 8.416855, adv: 0.594311, recon: 0.288102, id: 0.811187] time: 0:09:18.931854 \n",
      "[Epoch 1/20] [Batch 16/31] [D loss: 0.342839, acc:  48%] [G loss: 8.347606, adv: 0.702252, recon: 0.269357, id: 0.875242] time: 0:09:31.743822 \n",
      "[Epoch 1/20] [Batch 17/31] [D loss: 0.355905, acc:  41%] [G loss: 9.123447, adv: 0.967186, recon: 0.280621, id: 0.891482] time: 0:09:44.931866 \n",
      "[Epoch 1/20] [Batch 18/31] [D loss: 0.496144, acc:  33%] [G loss: 8.556500, adv: 0.594415, recon: 0.288994, id: 0.827729] time: 0:09:58.699763 \n",
      "[Epoch 1/20] [Batch 19/31] [D loss: 0.319307, acc:  49%] [G loss: 8.186979, adv: 0.678867, recon: 0.259921, id: 0.804679] time: 0:10:10.172545 \n",
      "[Epoch 1/20] [Batch 20/31] [D loss: 0.364146, acc:  45%] [G loss: 8.323384, adv: 0.708889, recon: 0.255904, id: 0.869053] time: 0:10:21.757102 \n",
      "Tensor(\"input_5:0\", shape=(?, 128, 128, 3), dtype=float32)\n",
      "[Epoch 1/20] [Batch 21/31] [D loss: 0.368495, acc:  44%] [G loss: 8.350412, adv: 0.677621, recon: 0.267317, id: 0.831601] time: 0:10:33.610776 \n",
      "[Epoch 1/20] [Batch 22/31] [D loss: 0.439825, acc:  35%] [G loss: 8.095992, adv: 0.599570, recon: 0.255383, id: 0.932447] time: 0:10:45.450967 \n",
      "[Epoch 1/20] [Batch 23/31] [D loss: 0.311643, acc:  50%] [G loss: 8.515981, adv: 0.745280, recon: 0.264910, id: 0.950590] time: 0:10:57.006841 \n",
      "[Epoch 1/20] [Batch 24/31] [D loss: 0.424854, acc:  34%] [G loss: 8.162635, adv: 0.651271, recon: 0.257425, id: 0.852500] time: 0:11:08.441248 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/20] [Batch 25/31] [D loss: 0.343576, acc:  50%] [G loss: 8.896963, adv: 1.105271, recon: 0.255960, id: 0.829949] time: 0:11:19.973290 \n",
      "[Epoch 1/20] [Batch 26/31] [D loss: 0.450318, acc:  44%] [G loss: 7.964173, adv: 0.586459, recon: 0.255217, id: 0.816267] time: 0:11:31.793912 \n",
      "[Epoch 1/20] [Batch 27/31] [D loss: 0.423925, acc:  34%] [G loss: 8.564905, adv: 0.792510, recon: 0.266553, id: 0.813380] time: 0:11:43.809087 \n",
      "[Epoch 1/20] [Batch 28/31] [D loss: 0.575580, acc:  23%] [G loss: 7.732025, adv: 0.640258, recon: 0.246682, id: 0.726477] time: 0:11:55.755244 \n",
      "[Epoch 1/20] [Batch 29/31] [D loss: 0.356946, acc:  46%] [G loss: 7.938466, adv: 0.858818, recon: 0.231849, id: 0.783448] time: 0:12:07.694241 \n",
      "[Epoch 2/20] [Batch 0/31] [D loss: 0.311734, acc:  54%] [G loss: 8.261071, adv: 0.713998, recon: 0.258658, id: 0.843827] time: 0:12:19.428878 \n",
      "Tensor(\"input_5:0\", shape=(?, 128, 128, 3), dtype=float32)\n",
      "[Epoch 2/20] [Batch 1/31] [D loss: 0.392425, acc:  36%] [G loss: 7.738689, adv: 0.682770, recon: 0.236484, id: 0.839724] time: 0:12:32.035217 \n",
      "[Epoch 2/20] [Batch 2/31] [D loss: 0.356242, acc:  42%] [G loss: 8.067120, adv: 0.884071, recon: 0.242061, id: 0.741344] time: 0:12:43.695677 \n",
      "[Epoch 2/20] [Batch 3/31] [D loss: 0.477780, acc:  34%] [G loss: 7.616107, adv: 0.583250, recon: 0.249101, id: 0.710974] time: 0:12:55.383110 \n",
      "[Epoch 2/20] [Batch 4/31] [D loss: 0.351664, acc:  39%] [G loss: 8.069962, adv: 0.757073, recon: 0.248687, id: 0.828898] time: 0:13:08.396086 \n",
      "[Epoch 2/20] [Batch 5/31] [D loss: 0.441481, acc:  32%] [G loss: 7.944796, adv: 0.712361, recon: 0.250305, id: 0.700520] time: 0:13:20.329869 \n",
      "[Epoch 2/20] [Batch 6/31] [D loss: 0.417155, acc:  37%] [G loss: 8.751723, adv: 1.046476, recon: 0.261288, id: 0.676724] time: 0:13:31.979607 \n",
      "[Epoch 2/20] [Batch 7/31] [D loss: 0.368539, acc:  59%] [G loss: 7.654365, adv: 0.680443, recon: 0.226341, id: 0.800063] time: 0:13:43.672019 \n",
      "[Epoch 2/20] [Batch 8/31] [D loss: 0.369116, acc:  44%] [G loss: 8.051550, adv: 0.739276, recon: 0.241551, id: 0.928942] time: 0:13:55.363887 \n",
      "[Epoch 2/20] [Batch 9/31] [D loss: 0.365373, acc:  37%] [G loss: 7.224097, adv: 0.595657, recon: 0.219559, id: 0.884863] time: 0:14:07.223943 \n",
      "[Epoch 2/20] [Batch 10/31] [D loss: 0.375481, acc:  37%] [G loss: 7.713843, adv: 0.709434, recon: 0.237510, id: 0.866286] time: 0:14:19.102871 \n",
      "Tensor(\"input_5:0\", shape=(?, 128, 128, 3), dtype=float32)\n",
      "[Epoch 2/20] [Batch 11/31] [D loss: 0.359554, acc:  44%] [G loss: 7.697924, adv: 0.699497, recon: 0.242135, id: 0.798684] time: 0:14:31.349796 \n",
      "[Epoch 2/20] [Batch 12/31] [D loss: 0.409578, acc:  37%] [G loss: 7.589061, adv: 0.713251, recon: 0.234283, id: 0.795317] time: 0:14:43.263335 \n",
      "[Epoch 2/20] [Batch 13/31] [D loss: 0.403897, acc:  37%] [G loss: 7.652442, adv: 0.601338, recon: 0.244448, id: 0.807180] time: 0:14:55.279298 \n",
      "[Epoch 2/20] [Batch 14/31] [D loss: 0.295193, acc:  47%] [G loss: 7.885205, adv: 0.847387, recon: 0.233797, id: 0.807934] time: 0:15:06.832081 \n",
      "[Epoch 2/20] [Batch 15/31] [D loss: 0.352031, acc:  51%] [G loss: 7.467450, adv: 0.682835, recon: 0.220052, id: 0.896520] time: 0:15:18.300361 \n",
      "[Epoch 2/20] [Batch 16/31] [D loss: 0.258067, acc:  60%] [G loss: 7.909161, adv: 0.899170, recon: 0.222563, id: 0.938313] time: 0:15:29.923968 \n",
      "[Epoch 2/20] [Batch 17/31] [D loss: 0.430181, acc:  39%] [G loss: 7.762782, adv: 0.633044, recon: 0.248253, id: 0.733630] time: 0:15:41.938609 \n",
      "[Epoch 2/20] [Batch 18/31] [D loss: 0.233161, acc:  67%] [G loss: 7.845039, adv: 0.861098, recon: 0.221465, id: 0.892807] time: 0:15:53.548587 \n",
      "[Epoch 2/20] [Batch 19/31] [D loss: 0.412344, acc:  35%] [G loss: 7.270617, adv: 0.604207, recon: 0.226878, id: 0.785128] time: 0:16:05.091294 \n",
      "[Epoch 2/20] [Batch 20/31] [D loss: 0.274302, acc:  53%] [G loss: 7.261341, adv: 0.833803, recon: 0.203237, id: 0.802519] time: 0:16:16.696609 \n",
      "Tensor(\"input_5:0\", shape=(?, 128, 128, 3), dtype=float32)\n",
      "[Epoch 2/20] [Batch 21/31] [D loss: 0.329425, acc:  54%] [G loss: 7.433191, adv: 0.704249, recon: 0.223091, id: 0.834817] time: 0:16:28.654243 \n",
      "[Epoch 2/20] [Batch 22/31] [D loss: 0.309809, acc:  52%] [G loss: 8.240850, adv: 0.945788, recon: 0.231091, id: 0.873363] time: 0:16:41.064066 \n",
      "[Epoch 2/20] [Batch 23/31] [D loss: 0.376855, acc:  44%] [G loss: 7.164180, adv: 0.563282, recon: 0.226927, id: 0.804947] time: 0:16:53.268874 \n",
      "[Epoch 2/20] [Batch 24/31] [D loss: 0.299912, acc:  48%] [G loss: 8.163471, adv: 1.001740, recon: 0.235429, id: 0.769361] time: 0:17:05.021327 \n",
      "[Epoch 2/20] [Batch 25/31] [D loss: 0.373771, acc:  50%] [G loss: 7.998178, adv: 0.669401, recon: 0.257020, id: 0.818840] time: 0:17:19.740182 \n",
      "[Epoch 2/20] [Batch 26/31] [D loss: 0.369698, acc:  49%] [G loss: 8.329581, adv: 1.047622, recon: 0.228450, id: 0.801080] time: 0:17:32.237772 \n",
      "[Epoch 2/20] [Batch 27/31] [D loss: 0.359916, acc:  58%] [G loss: 7.270173, adv: 0.727179, recon: 0.210176, id: 0.849428] time: 0:17:44.139913 \n",
      "[Epoch 2/20] [Batch 28/31] [D loss: 0.541374, acc:  30%] [G loss: 7.779817, adv: 0.581435, recon: 0.257713, id: 0.708586] time: 0:17:56.228024 \n",
      "[Epoch 2/20] [Batch 29/31] [D loss: 0.273608, acc:  57%] [G loss: 8.106806, adv: 1.113333, recon: 0.209755, id: 0.839349] time: 0:18:08.314489 \n",
      "[Epoch 3/20] [Batch 0/31] [D loss: 0.554836, acc:  31%] [G loss: 6.794689, adv: 0.604035, recon: 0.198332, id: 0.816561] time: 0:18:20.170423 \n",
      "Tensor(\"input_5:0\", shape=(?, 128, 128, 3), dtype=float32)\n",
      "[Epoch 3/20] [Batch 1/31] [D loss: 0.351793, acc:  46%] [G loss: 7.114543, adv: 0.729210, recon: 0.203636, id: 0.777280] time: 0:18:32.091990 \n",
      "[Epoch 3/20] [Batch 2/31] [D loss: 0.355040, acc:  48%] [G loss: 7.222685, adv: 0.791927, recon: 0.203943, id: 0.754831] time: 0:18:43.847597 \n",
      "[Epoch 3/20] [Batch 3/31] [D loss: 0.322092, acc:  50%] [G loss: 6.833637, adv: 0.674974, recon: 0.197491, id: 0.808317] time: 0:18:55.445481 \n",
      "[Epoch 3/20] [Batch 4/31] [D loss: 0.399831, acc:  35%] [G loss: 7.060727, adv: 0.619884, recon: 0.221428, id: 0.700855] time: 0:19:07.165668 \n",
      "[Epoch 3/20] [Batch 5/31] [D loss: 0.253781, acc:  62%] [G loss: 7.644959, adv: 0.803472, recon: 0.224012, id: 0.699612] time: 0:19:18.809701 \n",
      "[Epoch 3/20] [Batch 6/31] [D loss: 0.291622, acc:  51%] [G loss: 7.359162, adv: 0.724056, recon: 0.219642, id: 0.810292] time: 0:19:30.363226 \n",
      "[Epoch 3/20] [Batch 7/31] [D loss: 0.264832, acc:  60%] [G loss: 7.932875, adv: 0.856423, recon: 0.236247, id: 0.760964] time: 0:19:42.092412 \n",
      "[Epoch 3/20] [Batch 8/31] [D loss: 0.308951, acc:  51%] [G loss: 7.075102, adv: 0.752514, recon: 0.199873, id: 0.822447] time: 0:19:53.860863 \n",
      "[Epoch 3/20] [Batch 9/31] [D loss: 0.310418, acc:  53%] [G loss: 6.948941, adv: 0.644646, recon: 0.208234, id: 0.839343] time: 0:20:05.414507 \n",
      "[Epoch 3/20] [Batch 10/31] [D loss: 0.274279, acc:  57%] [G loss: 7.360115, adv: 0.839845, recon: 0.205894, id: 0.875808] time: 0:20:17.175613 \n",
      "Tensor(\"input_5:0\", shape=(?, 128, 128, 3), dtype=float32)\n",
      "[Epoch 3/20] [Batch 11/31] [D loss: 0.359613, acc:  51%] [G loss: 6.846305, adv: 0.830175, recon: 0.180511, id: 0.845190] time: 0:20:29.094339 \n",
      "[Epoch 3/20] [Batch 12/31] [D loss: 0.358840, acc:  46%] [G loss: 7.315654, adv: 0.748682, recon: 0.218635, id: 0.829196] time: 0:20:40.699270 \n",
      "[Epoch 3/20] [Batch 13/31] [D loss: 0.316010, acc:  50%] [G loss: 7.318031, adv: 0.844557, recon: 0.202091, id: 0.808537] time: 0:20:52.560539 \n",
      "[Epoch 3/20] [Batch 14/31] [D loss: 0.300683, acc:  57%] [G loss: 7.182538, adv: 0.688550, recon: 0.206488, id: 0.846627] time: 0:21:05.295543 \n",
      "[Epoch 3/20] [Batch 15/31] [D loss: 0.397277, acc:  38%] [G loss: 7.151991, adv: 0.661689, recon: 0.212109, id: 0.811719] time: 0:21:18.934686 \n",
      "[Epoch 3/20] [Batch 16/31] [D loss: 0.339565, acc:  47%] [G loss: 6.680969, adv: 0.660205, recon: 0.192268, id: 0.777394] time: 0:21:32.368947 \n",
      "[Epoch 3/20] [Batch 17/31] [D loss: 0.284924, acc:  56%] [G loss: 6.877186, adv: 0.709639, recon: 0.193821, id: 0.869507] time: 0:21:44.540274 \n",
      "[Epoch 3/20] [Batch 18/31] [D loss: 0.272850, acc:  54%] [G loss: 7.084936, adv: 0.798488, recon: 0.199717, id: 0.850372] time: 0:21:56.317355 \n",
      "[Epoch 3/20] [Batch 19/31] [D loss: 0.364908, acc:  44%] [G loss: 7.294598, adv: 0.634663, recon: 0.219013, id: 0.855716] time: 0:22:07.987629 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/20] [Batch 20/31] [D loss: 0.395737, acc:  31%] [G loss: 7.209821, adv: 0.703001, recon: 0.212393, id: 0.827724] time: 0:22:20.132122 \n",
      "Tensor(\"input_5:0\", shape=(?, 128, 128, 3), dtype=float32)\n",
      "[Epoch 3/20] [Batch 21/31] [D loss: 0.267652, acc:  56%] [G loss: 7.849110, adv: 0.891619, recon: 0.221161, id: 0.854983] time: 0:22:32.098434 \n",
      "[Epoch 3/20] [Batch 22/31] [D loss: 0.365695, acc:  44%] [G loss: 7.239266, adv: 0.856749, recon: 0.203164, id: 0.692622] time: 0:22:43.845054 \n",
      "[Epoch 3/20] [Batch 23/31] [D loss: 0.344024, acc:  54%] [G loss: 6.874855, adv: 0.676638, recon: 0.194480, id: 0.847280] time: 0:22:55.623637 \n",
      "[Epoch 3/20] [Batch 24/31] [D loss: 0.264352, acc:  56%] [G loss: 7.372781, adv: 0.799683, recon: 0.210817, id: 0.747579] time: 0:23:07.308558 \n",
      "[Epoch 3/20] [Batch 25/31] [D loss: 0.300429, acc:  54%] [G loss: 6.905420, adv: 0.728874, recon: 0.202249, id: 0.707569] time: 0:23:19.077238 \n",
      "[Epoch 3/20] [Batch 26/31] [D loss: 0.343854, acc:  49%] [G loss: 7.885133, adv: 0.917368, recon: 0.221600, id: 0.808006] time: 0:23:30.696818 \n",
      "[Epoch 3/20] [Batch 27/31] [D loss: 0.402857, acc:  46%] [G loss: 7.252635, adv: 0.655363, recon: 0.215341, id: 0.924738] time: 0:23:42.439589 \n",
      "[Epoch 3/20] [Batch 28/31] [D loss: 0.237577, acc:  63%] [G loss: 7.442546, adv: 0.886131, recon: 0.208741, id: 0.811693] time: 0:23:54.196185 \n",
      "[Epoch 3/20] [Batch 29/31] [D loss: 0.332965, acc:  53%] [G loss: 7.225718, adv: 0.768673, recon: 0.206565, id: 0.761819] time: 0:24:05.886536 \n",
      "[Epoch 4/20] [Batch 0/31] [D loss: 0.240777, acc:  69%] [G loss: 6.885381, adv: 0.846851, recon: 0.186471, id: 0.772842] time: 0:24:17.624304 \n",
      "Tensor(\"input_5:0\", shape=(?, 128, 128, 3), dtype=float32)\n",
      "[Epoch 4/20] [Batch 1/31] [D loss: 0.397010, acc:  44%] [G loss: 6.944716, adv: 0.717739, recon: 0.205030, id: 0.662128] time: 0:24:29.612475 \n",
      "[Epoch 4/20] [Batch 2/31] [D loss: 0.249318, acc:  63%] [G loss: 7.779447, adv: 1.076605, recon: 0.212054, id: 0.728633] time: 0:24:41.422098 \n",
      "[Epoch 4/20] [Batch 3/31] [D loss: 0.428503, acc:  45%] [G loss: 6.995181, adv: 0.809755, recon: 0.191025, id: 0.823234] time: 0:24:53.515114 \n",
      "[Epoch 4/20] [Batch 4/31] [D loss: 0.362957, acc:  56%] [G loss: 7.190061, adv: 0.735114, recon: 0.203770, id: 0.826503] time: 0:25:05.163180 \n",
      "[Epoch 4/20] [Batch 5/31] [D loss: 0.351020, acc:  47%] [G loss: 7.314681, adv: 0.793404, recon: 0.214422, id: 0.680639] time: 0:25:16.909319 \n"
     ]
    }
   ],
   "source": [
    "train(epochs=20, batch_size=10, sample_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
